{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-bl6HsY9vVjR"
   },
   "outputs": [],
   "source": [
    "# Library to suppress warnings or deprecation notes \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L0KUYAJYvee7"
   },
   "outputs": [],
   "source": [
    "def clean_weather_column_names(df):\n",
    "    '''Strip out blanks in weather dataframe columns\n",
    "        :df: DataFrame to use\n",
    "    '''\n",
    "    df.rename(columns=lambda x: x.strip(), inplace = True)\n",
    "    return df\n",
    "\n",
    "def create_single_weather_file(directory, prefix, suffix = '*.txt', lines_to_skip=19):\n",
    "    '''Read all the weather files in a directory and create a concatenated dataframe\n",
    "        :directory: Directory with weather file\n",
    "        :prefix: The prefix of the weather file to look for in the directory\n",
    "        :suffix: The suffix of the weather file to look for in the directory default is *.txt\n",
    "        :lines_to_skip: Number of lines to skip in the files before csv header default is 19\n",
    "    '''\n",
    "    directory_mask = os.path.join(directory, prefix + suffix)\n",
    "    \n",
    "    # Find list of matching filenames in the directory\n",
    "    filenames = glob.glob(directory_mask)\n",
    "    concat_df = None\n",
    "    \n",
    "    if (len(filenames) > 0):\n",
    "        df_list = []\n",
    "        for filename in filenames:\n",
    "            df_list.append(pd.read_csv(filename, skiprows=lines_to_skip))\n",
    "\n",
    "        concat_df = pd.concat(df_list, axis=0)\n",
    "        concat_df = clean_weather_column_names(concat_df)\n",
    "    else:\n",
    "        print('No files found in {} for prefix {}'.format(directory, prefix))\n",
    "        \n",
    "    return concat_df\n",
    "\n",
    "def process_weather_files(root_directory, output_directory, min_date='2016-12-31', keep_valid_only=True):\n",
    "    '''Process all the weather file directories\n",
    "        :root_directory: Root directory containing the weather file subdirectories\n",
    "        :output_directory: Directory to output merged files\n",
    "        :min_date: The min date to exclude from the date range after building the combined file default is 12/31/2016\n",
    "    '''\n",
    "    \n",
    "    # Mapping between leaf names and file name prefix in child directory\n",
    "    directory_dict = {'ECA_cloud_cover': 'CC','ECA_global_radiation': 'QQ','ECA_humidity': 'HU',\n",
    "                'ECA_mean_temperature': 'TG','ECA_precipitation': 'RR','ECA_sea_level_pressure': 'PP',\n",
    "                'ECA_snow depth': 'SD','ECA_sunshine': 'SS','ECA_wind_speed': 'FG'}\n",
    "    \n",
    "    for key, value in tqdm(directory_dict.items()):\n",
    "        # Concatenate files into a single dateframe\n",
    "        current_directory = os.path.join(root_directory, key)\n",
    "        prefix = value\n",
    "        \n",
    "        #Create dataframe for the sources.txt file\n",
    "        #Number of lines to skip in the sources.txt files before csv header is 23\n",
    "        sources_df = pd.read_csv(os.path.join(current_directory, 'sources.txt'), skiprows=23)\n",
    "        sources_df.rename(columns=lambda x: x.strip(), inplace = True) #strip extra white space from column names\n",
    "        sources_df['BEGIN'] = pd.to_datetime(sources_df['BEGIN'].astype(str)) #format the date columns\n",
    "        sources_df['END'] = pd.to_datetime(sources_df['END'].astype(str)) #format the date columns\n",
    "        sources_df['SOUNAME'] = sources_df['SOUNAME'].str.strip() #strip leading / trailing white spaces from the column\n",
    "        sources_df['PARNAME'] = sources_df['PARNAME'].str.strip() #strip leading / trailing white spaces from the column\n",
    "        sources_df['PARID'] = sources_df['PARID'].str.strip() #strip leading / trailing white spaces from the column\n",
    "\n",
    "        # Create a single weather file\n",
    "        combined_df = create_single_weather_file(current_directory, prefix)\n",
    "\n",
    "        # Convert DATE column to datetime so we can filter 2017-2021\n",
    "        combined_df['DATE'] = pd.to_datetime(combined_df['DATE'].astype(str))\n",
    "        combined_df = combined_df[combined_df['DATE'] > min_date]\n",
    "        \n",
    "        # Ignore bad readings and merge with source data to get lat/long\n",
    "        if (keep_valid_only):\n",
    "            combined_df = combined_df[combined_df['Q_' + value] == 0]\n",
    "        \n",
    "        combined_df = pd.merge(combined_df,sources_df, how='inner', on='SOUID')\n",
    "        \n",
    "        # I found duplicated data in source weather files. Ensure its removed.\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "        \n",
    "        # See if we have an duplicated data\n",
    "        print('Rows of duplicated data in the dataset {}'.format(combined_df.duplicated().sum()))\n",
    "        \n",
    "        # output a single file\n",
    "        output_file = os.path.join(output_directory, 'Combined_{}_{}.csv'.format(prefix, key))\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "        #print(sources_df.head())\n",
    "        #sources_output_file = os.path.join(output_directory, 'sources_{}_{}.csv'.format(prefix, key))\n",
    "        #sources_df.to_csv(sources_output_file, index=False)\n",
    "\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q7zUwtdnvhDS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 1/9 [00:05<00:40,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [00:07<00:22,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 3/9 [00:10<00:19,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 4/9 [00:15<00:19,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 5/9 [00:20<00:17,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 6/9 [00:22<00:10,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 7/9 [00:27<00:07,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 8/9 [00:29<00:03,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:34<00:00,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the weather files from zip\n",
    "#root_directory = '/home/rukshar/Documents/Omdena/Poland/Air Quality/Data/daily_weather_data/daily_weather_data_1979-2021'\n",
    "root_directory = 'C:/Users/thayes/omdena/Air Pollution in Poland/daily_weather_data_1979-2021'\n",
    "\n",
    "# Output directory for combined file\n",
    "#output_directory = '/home/rukshar/Documents/Omdena/Poland/Air Quality/Data/processed_weather_data'\n",
    "output_directory = 'c:/temp/processed_weather_data'\n",
    "\n",
    "process_weather_files(root_directory, output_directory)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN32EKraw5LXcsP1nzXdEii",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
