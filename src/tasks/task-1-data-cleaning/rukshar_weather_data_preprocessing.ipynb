{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-bl6HsY9vVjR"
   },
   "outputs": [],
   "source": [
    "# Library to suppress warnings or deprecation notes \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L0KUYAJYvee7"
   },
   "outputs": [],
   "source": [
    "def clean_weather_column_names(df):\n",
    "    '''Strip out blanks in weather dataframe columns\n",
    "        :df: DataFrame to use\n",
    "    '''\n",
    "    df.rename(columns=lambda x: x.strip(), inplace = True)\n",
    "    return df\n",
    "\n",
    "def create_single_weather_file(directory, prefix, suffix = '*.txt', lines_to_skip=19):\n",
    "    '''Read all the weather files in a directory and create a concatenated dataframe\n",
    "        :directory: Directory with weather file\n",
    "        :prefix: The prefix of the weather file to look for in the directory\n",
    "        :suffix: The suffix of the weather file to look for in the directory default is *.txt\n",
    "        :lines_to_skip: Number of lines to skip in the files before csv header default is 19\n",
    "    '''\n",
    "    directory_mask = os.path.join(directory, prefix + suffix)\n",
    "    \n",
    "    # Find list of matching filenames in the directory\n",
    "    filenames = glob.glob(directory_mask)\n",
    "    concat_df = None\n",
    "    \n",
    "    if (len(filenames) > 0):\n",
    "        df_list = []\n",
    "        for filename in filenames:\n",
    "            staid = int(filename[len(directory_mask)-4:-4].strip('STAID'))\n",
    "            idf = pd.read_csv(filename, skiprows=lines_to_skip)\n",
    "            idf['STAID'] = staid\n",
    "            #moving STAID column to be the first \n",
    "            cols = list(idf.columns)\n",
    "            cols = [cols[-1]] + cols[:-1]\n",
    "            idf = idf[cols]\n",
    "            df_list.append(idf)\n",
    "            # df_list.append(pd.read_csv(filename, skiprows=lines_to_skip))\n",
    "\n",
    "        concat_df = pd.concat(df_list, axis=0)\n",
    "        concat_df = clean_weather_column_names(concat_df)\n",
    "    else:\n",
    "        print('No files found in {} for prefix {}'.format(directory, prefix))\n",
    "        \n",
    "    return concat_df\n",
    " \n",
    "def process_weather_files(root_directory, df_stations, output_directory, min_date='2016-12-31'):\n",
    "    '''Process all the weather file directories\n",
    "        :root_directory: Root directory containing the weather file subdirectories\n",
    "        :stations_df Dataframe with formatted lat/long\n",
    "        :output_directory: Directory to output merged files\n",
    "        :min_date: The min date to exclude from the date range after building the combined file default is 12/31/2016\n",
    "    '''\n",
    "    \n",
    "    # Mapping between leaf names and file name prefix in child directory\n",
    "    directory_dict = {'ECA_cloud_cover': 'CC','ECA_global_radiation': 'QQ','ECA_humidity': 'HU',\n",
    "                'ECA_mean_temperature': 'TG','ECA_precipitation': 'RR','ECA_sea_level_pressure': 'PP',\n",
    "                'ECA_snow depth': 'SD','ECA_sunshine': 'SS','ECA_wind_speed': 'FG'}\n",
    "    \n",
    "    for key, value in tqdm(directory_dict.items()):\n",
    "        # Concatenate files into a single dateframe\n",
    "        current_directory = os.path.join(root_directory, key)\n",
    "        prefix = value\n",
    "        \n",
    "        #Create dataframe for the sources.txt file\n",
    "        #Number of lines to skip in the sources.txt files before csv header is 23\n",
    "        sources_df = pd.read_csv(os.path.join(current_directory, 'sources.txt'), skiprows=23,\n",
    "                                usecols = ['STAID',' SOUID', 'SOUNAME                                 '])\n",
    "        sources_df.rename(columns=lambda x: x.strip(), inplace = True) #strip extra white space from column names\n",
    "        sources_df['SOUNAME'] = sources_df['SOUNAME'].str.strip() #strip leading / trailing white spaces from the column\n",
    "        \n",
    "        # STAID has a one to many relationship with SOUID. Go ahead and drop dupe rows\n",
    "        # sources_df = sources_df.drop_duplicates(subset=['SOUID'])\n",
    "\n",
    "        # Create a single weather file\n",
    "        combined_df = create_single_weather_file(current_directory, prefix)\n",
    "\n",
    "        # Drop duplicates I found the source weather data contains dupes\n",
    "        combined_df = combined_df.drop_duplicates()\n",
    "        \n",
    "        # Convert DATE column to datetime so we can filter 2017-2021\n",
    "        combined_df['DATE'] = pd.to_datetime(combined_df['DATE'].astype(str))\n",
    "        combined_df = combined_df[combined_df['DATE'] > min_date]\n",
    "        \n",
    "        #\n",
    "        # DO NOT MESS WITH Quality column removal because it impacts merging datasets. Let\n",
    "        # this be handled later\n",
    "        #\n",
    "        \n",
    "        # Ignore bad readings and merge with source data to get lat/long\n",
    "        #combined_df = combined_df[combined_df['Q_' + value] == 0]\n",
    "        \n",
    "        # Drop the quality column since we are only looking at non-suspect data\n",
    "        #combined_df.drop('Q_' + value, axis=1, inplace=True)\n",
    "                           \n",
    "        # Merge with sources df based on SOIID\n",
    "        # combined_df = pd.merge(combined_df,sources_df, how='inner', on=['SOUID','STAID'])\n",
    "        \n",
    "        # I found duplicated data in source weather files. Ensure its removed.\n",
    "        # combined_df = pd.merge(combined_df, df_stations, how='inner', on='STAID')\n",
    "        \n",
    "        # See if we have an duplicated data\n",
    "        print('Rows of duplicated data in the dataset {}'.format(combined_df.duplicated().sum()))\n",
    "        \n",
    "        # output a single file\n",
    "        output_file = os.path.join(output_directory, 'Combined_{}_{}.csv'.format(prefix, key))\n",
    "        combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Q7zUwtdnvhDS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:03<00:10,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n",
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:04<00:09,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:06<00:08,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:09<00:04,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n",
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:12<00:01,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows of duplicated data in the dataset 0\n",
      "Rows of duplicated data in the dataset 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:14<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the weather files from zip\n",
    "#root_directory = '/home/rukshar/Documents/Omdena/Poland/Air Quality/Data/daily_weather_data/daily_weather_data_1979-2021'\n",
    "# root_directory = 'C:/Users/thayes/omdena/warsaw-poland-chapter-air-pollution/src/data/data_raw/daily_weather_data_1979-2021'\n",
    "root_directory = \"../../data/1. data_raw/daily_weather_data_1979-2021\"\n",
    "\n",
    "# Output directory for combined file\n",
    "#output_directory = '/home/rukshar/Documents/Omdena/Poland/Air Quality/Data/processed_weather_data'\n",
    "# output_directory = 'c:/temp/processed_weather_data'\n",
    "output_directory = \"../../data/4. data_processed_1/processed_weather_data_in_CSV_files\"\n",
    "\n",
    "# stations = os.path.join(root_directory, 'stations_with_powiat_voivod_GEOJSON.csv')\n",
    "stations = \"../../data/4. data_processed_1/processed_weather_data_in_CSV_files/stations_with_powiat_voivod_GEOJSON.csv\"\n",
    "df_stations = pd.read_csv(stations, usecols = ['STAID','LAT', 'LON'])\n",
    "\n",
    "process_weather_files(root_directory, df_stations, output_directory)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN32EKraw5LXcsP1nzXdEii",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
